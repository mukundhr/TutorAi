LLAMA_MODEL_PATH=./models/llama-2-7b-chat.Q4_K_M.gguf

# Model configuration
MODEL_CONTEXT_LENGTH=4096
MODEL_MAX_TOKENS=512
MODEL_TEMPERATURE=0.7

# Question generation settings
DEFAULT_NUM_SAQ=5
DEFAULT_NUM_MCQ=5

# Enable/disable evaluation metrics
ENABLE_BERTSCORE=true
ENABLE_ROUGE=true